{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "031be095-e39e-41a5-98cf-a989567b6933",
      "metadata": {
        "id": "031be095-e39e-41a5-98cf-a989567b6933"
      },
      "outputs": [],
      "source": [
        "# !pip install -U langchain-community pypdf transformers nltk sentence_transformers faiss-cpu numpy langchain_groq gradio neo4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6312bb6-c00c-47a4-885f-b552f3b3c140",
      "metadata": {
        "id": "c6312bb6-c00c-47a4-885f-b552f3b3c140"
      },
      "outputs": [],
      "source": [
        "# !pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d63a49d0-9ae2-4208-955f-d1dadb775674",
      "metadata": {
        "id": "d63a49d0-9ae2-4208-955f-d1dadb775674"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "pdf_paths = [\"/BNS.pdf\"]\n",
        "\n",
        "class PDFFunctions():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def pdf_to_chunks(self, pdf_paths):\n",
        "    # pdf to page\n",
        "    documents = []\n",
        "    for x in pdf_paths:\n",
        "      loader = PyPDFLoader(x)\n",
        "      documents.extend(loader.load())\n",
        "\n",
        "    # tokenize\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=300\n",
        "    )\n",
        "    chunks = splitter.split_documents(documents)\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cc91700-01d3-4305-b735-e87376ce5fbb",
      "metadata": {
        "id": "3cc91700-01d3-4305-b735-e87376ce5fbb"
      },
      "outputs": [],
      "source": [
        "# from sentence_transformers import SentenceTransformer\n",
        "# from chromadb import PersistentClient\n",
        "\n",
        "# class Vectorize():\n",
        "#     def __init__(self, persist_directory=\"./chroma_db\", collection_name=\"my_collection\"):\n",
        "#         # Load embedding model once\n",
        "#         self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "#         # New ChromaDB persistent client\n",
        "#         self.client = PersistentClient(path=persist_directory)\n",
        "\n",
        "#         # Create or get the collection\n",
        "#         self.collection = self.client.get_or_create_collection(name=collection_name)\n",
        "\n",
        "#     def documents_to_vector(self, documents):\n",
        "#         texts = [doc.page_content for doc in documents]\n",
        "#         ids = [f\"doc_{i}\" for i in range(len(texts))]\n",
        "\n",
        "#         embeddings = self.model.encode(texts, convert_to_numpy=True).tolist()\n",
        "\n",
        "#         self.collection.add(\n",
        "#             documents=texts,\n",
        "#             embeddings=embeddings,\n",
        "#             ids=ids\n",
        "#         )\n",
        "\n",
        "#         return self.collection\n",
        "\n",
        "#     def top_k(self, query, k=5):\n",
        "#         results = self.collection.query(\n",
        "#             query_texts=[query],\n",
        "#             n_results=k\n",
        "#         )\n",
        "#         top_texts = results.get('documents', [[]])[0]\n",
        "#         return \"\\n\".join(top_texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3281de78-5fe3-4b22-97ef-67dc728e5403",
      "metadata": {
        "id": "3281de78-5fe3-4b22-97ef-67dc728e5403"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import re\n",
        "import time\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "class Inference:\n",
        "    def __init__(self, model=\"deepseek-r1-distill-llama-70b\", api_key=\"\", batch_size=10):\n",
        "        self.llm = ChatGroq(model=model, api_key=api_key)\n",
        "        self.batch_size = batch_size\n",
        "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Extract legal information tuples from the following text.\n",
        "Return ONLY a valid Python list of tuples in this format:\n",
        "\n",
        "(offence, chapter, section, punishment_clause)\n",
        "\n",
        "Here are some detailed examples:\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"\n",
        "1. Offence: Theft; Chapter: 5; Section: 378; Punishment: Imprisonment up to 3 years or fine, or both.\n",
        "2. Offence: Criminal Breach of Trust; Chapter: 7; Section: 405; Punishment: Imprisonment for up to 2 years, or fine, or both.\n",
        "\\\"\\\"\\\"\n",
        "Output:\n",
        "[\n",
        "    (\"Theft\", \"5\", \"378\", \"Imprisonment up to 3 years or fine, or both\"),\n",
        "    (\"Criminal Breach of Trust\", \"7\", \"405\", \"Imprisonment for up to 2 years, or fine, or both\")\n",
        "]\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"\n",
        "The offence of Murder falls under Chapter 6, Section 302. The punishment prescribed is death penalty or life imprisonment.\n",
        "Attempt to murder is dealt with in Chapter 6, Section 307, punishable by imprisonment for up to 10 years and fine.\n",
        "\\\"\\\"\\\"\n",
        "Output:\n",
        "[\n",
        "    (\"Murder\", \"6\", \"302\", \"Death penalty or life imprisonment\"),\n",
        "    (\"Attempt to murder\", \"6\", \"307\", \"Imprisonment for up to 10 years and fine\")\n",
        "]\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"\n",
        "Offence: Cheating; Chapter: 8; Section: 420; Punishment: Imprisonment which may extend to seven years and fine.\n",
        "\\\"\\\"\\\"\n",
        "Output:\n",
        "[\n",
        "    (\"Cheating\", \"8\", \"420\", \"Imprisonment which may extend to seven years and fine\")\n",
        "]\n",
        "\n",
        "Now extract from the following text:\n",
        "\n",
        "\\\"\\\"\\\"{input_text}\\\"\\\"\\\"\n",
        "\"\"\")\n",
        "\n",
        "    def _safe_llm_invoke(self, prompt):\n",
        "        for _ in range(3):\n",
        "            try:\n",
        "                return self.llm.invoke(prompt)\n",
        "            except Exception:\n",
        "                time.sleep(1)\n",
        "        return None\n",
        "\n",
        "    def extract_custom_tuples(self, chunks):\n",
        "        all_tuples = []\n",
        "        total_batches = (len(chunks) + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "        for batch_idx in range(0, len(chunks), self.batch_size):\n",
        "            batch_chunks = chunks[batch_idx:batch_idx + self.batch_size]\n",
        "\n",
        "            combined_text = \"\\n\\n\".join(\n",
        "                [f\"[CHUNK {i + batch_idx + 1}]\\n{chunk.page_content}\" for i, chunk in enumerate(batch_chunks)]\n",
        "            )\n",
        "\n",
        "            formatted_prompt = self.prompt.format_prompt(input_text=combined_text).to_messages()\n",
        "            response = self._safe_llm_invoke(formatted_prompt)\n",
        "            if not response:\n",
        "                continue\n",
        "\n",
        "            output_str = response.content\n",
        "\n",
        "            # Regex to extract Python lists of tuples\n",
        "            matches = re.findall(r\"\\[\\s*\\([^)]+\\)\\s*(?:,\\s*\\([^)]+\\)\\s*)*\\]\", output_str, re.DOTALL)\n",
        "            batch_tuples = []\n",
        "\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    tuples = ast.literal_eval(match)\n",
        "                    if isinstance(tuples, list) and all(len(t) == 4 for t in tuples):\n",
        "                        batch_tuples.extend(tuples)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            all_tuples.extend(batch_tuples)\n",
        "            print(f\"Batch {(batch_idx // self.batch_size) + 1} finished\")\n",
        "\n",
        "        unique_tuples = list(set(all_tuples))\n",
        "        return unique_tuples\n",
        "\n",
        "    def save_tuples_to_file(self, tuples, filename=\"graphData.txt\"):\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"[\\n\")\n",
        "            for t in tuples:\n",
        "                f.write(f\"    {repr(t)},\\n\")\n",
        "            f.write(\"]\\n\")\n",
        "\n",
        "    def load_tuples_from_file(self, filename=\"graphData.txt\"):\n",
        "        \"\"\"\n",
        "        Load list of tuples from a file containing a Python list literal.\n",
        "        \"\"\"\n",
        "\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "            try:\n",
        "                tuples = ast.literal_eval(content)\n",
        "                if isinstance(tuples, list) and all(isinstance(t, tuple) for t in tuples):\n",
        "                    return tuples\n",
        "                else:\n",
        "                    print(\"[Warning] File content is not a list of tuples.\")\n",
        "                    return []\n",
        "            except Exception as e:\n",
        "                print(f\"[Error] Failed to parse file '{filename}': {e}\")\n",
        "                return []\n",
        "\n",
        "    def answer_user_question(self, context_for_llm, matched_node):\n",
        "      \"\"\"\n",
        "      Given legal context and a matched node (offense), prompt the LLM to extract\n",
        "      relevant legal references: chapter, section, and punishment clause from the BNS.\n",
        "      \"\"\"\n",
        "      prompt = f\"\"\"\n",
        "    You are a legal assistant AI trained to interpret and extract structured legal information from the Bharatiya Nyaya Sanhita (BNS) based on a legal knowledge graph.\n",
        "\n",
        "    Given the retrieved legal graph context and the user's target offense or concept, return the relevant:\n",
        "    - **Chapter Number and Name**\n",
        "    - **Section Number and Title**\n",
        "    - **Punishment Clause(s)**\n",
        "    - **Any Directly Related Offenses**\n",
        "\n",
        "    Ensure the answer is structured, cited exactly as in BNS, and only use information from the retrieved context.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Example 1:\n",
        "\n",
        "    **Matched Node:** Theft\n",
        "\n",
        "    **Retrieved Context:**\n",
        "    Chapter XVII – Of Offenses Against Property\n",
        "    Section 303 – Theft\n",
        "    Whoever commits theft shall be punished with imprisonment of either description for a term which may extend to three years, or with fine, or with both.\n",
        "\n",
        "    **Answer:**\n",
        "    - Chapter: Chapter XVII – Of Offenses Against Property\n",
        "    - Section: Section 303 – Theft\n",
        "    - Punishment: Imprisonment up to 3 years, or fine, or both\n",
        "    - Related Offenses: Attempt to commit theft, aggravated theft under Section 304\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Example 2:\n",
        "\n",
        "    **Matched Node:** Dacoity\n",
        "\n",
        "    **Retrieved Context:**\n",
        "    Chapter XVII – Of Offenses Against Property\n",
        "    Section 310 – Dacoity\n",
        "    When five or more persons conjointly commit or attempt to commit a robbery, it is called “dacoity”. Punishment is imprisonment for life, or rigorous imprisonment for not less than 10 years.\n",
        "\n",
        "    **Answer:**\n",
        "    - Chapter: Chapter XVII – Of Offenses Against Property\n",
        "    - Section: Section 310 – Dacoity\n",
        "    - Punishment: Life imprisonment or rigorous imprisonment of not less than 10 years\n",
        "    - Related Offenses: Robbery (Section 309), preparation to commit dacoity (Section 311)\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### User Query:\n",
        "\n",
        "    **Matched Node:** {matched_node}\n",
        "\n",
        "    **Retrieved Context:**\n",
        "    {context_for_llm}\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Answer:\n",
        "    \"\"\"\n",
        "\n",
        "      response = self.llm.invoke(prompt)\n",
        "      return response.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5403b24-da08-4310-85d7-f3923e6d81ef",
      "metadata": {
        "id": "c5403b24-da08-4310-85d7-f3923e6d81ef"
      },
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine\n",
        "import numpy as np\n",
        "\n",
        "class Graphclass:\n",
        "    def __init__(self, database=\"neo4j\"):\n",
        "        self.uri = \"\"\n",
        "        self.user = \"\"\n",
        "        self.password = \"\"\n",
        "        self.database = database\n",
        "        self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.password))\n",
        "\n",
        "        # Initialize embedding model once\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Placeholder for cached node names and embeddings\n",
        "        self.node_names = []\n",
        "        self.node_embeddings = None\n",
        "\n",
        "    def create_knowledge_graph(self, tuples):\n",
        "        \"\"\"\n",
        "        tuples: List of (offence, chapter, section, punishment_clause)\n",
        "        \"\"\"\n",
        "        with self.driver.session(database=self.database) as session:\n",
        "            for i, (offence, chapter, section, punishment) in enumerate(tuples):\n",
        "                try:\n",
        "                    session.execute_write(\n",
        "                        self._create_offense_graph,\n",
        "                        offence, chapter, section, punishment\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"[Warning] Failed to create graph for tuple {i}: {offence, chapter, section, punishment}\")\n",
        "                    print(\"  Reason:\", e)\n",
        "        print(\"Graph creation complete.\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_offense_graph(tx, offence, chapter, section, punishment):\n",
        "        query = \"\"\"\n",
        "  MERGE (o:Offense {name: $offence})\n",
        "  MERGE (c:Chapter {name: 'Chapter No.: ' + $chapter})\n",
        "  MERGE (s:Section {number: 'Section No.: ' + $section})\n",
        "  MERGE (p:Punishment {description: $punishment})\n",
        "\n",
        "  MERGE (o)-[:refersToChapter]->(c)\n",
        "  MERGE (o)-[:refersToSection]->(s)\n",
        "  MERGE (o)-[:hasPunishment]->(p)\n",
        "  \"\"\"\n",
        "        tx.run(query, offence=offence, chapter=chapter, section=section, punishment=punishment)\n",
        "\n",
        "    def _fetch_all_node_names(self):\n",
        "        \"\"\"Fetch distinct node names from the Neo4j graph.\"\"\"\n",
        "        def fetch_all_node_names(tx):\n",
        "            query = \"\"\"\n",
        "            MATCH (n)\n",
        "            WHERE n.name IS NOT NULL\n",
        "            RETURN DISTINCT n.name AS name\n",
        "            \"\"\"\n",
        "            result = tx.run(query)\n",
        "            return [record[\"name\"] for record in result]\n",
        "\n",
        "        with self.driver.session(database=self.database) as session:\n",
        "            node_names = session.execute_read(fetch_all_node_names)\n",
        "\n",
        "        # Convert all to strings to be safe\n",
        "        self.node_names = [str(name) for name in node_names]\n",
        "\n",
        "    def _encode_node_names(self):\n",
        "        \"\"\"Encode all node names to embeddings.\"\"\"\n",
        "        if not self.node_names:\n",
        "            self._fetch_all_node_names()\n",
        "        self.node_embeddings = self.model.encode(self.node_names, convert_to_numpy=True)\n",
        "\n",
        "    def find_most_similar_node(self, input_text):\n",
        "        \"\"\"\n",
        "        Given input_text, find the most similar node name in the graph.\n",
        "        Returns: (best_node_name, similarity_score)\n",
        "        \"\"\"\n",
        "        if self.node_embeddings is None:\n",
        "            self._encode_node_names()\n",
        "\n",
        "        input_embedding = self.model.encode([input_text], convert_to_numpy=True)[0]\n",
        "\n",
        "        similarities = [1 - cosine(input_embedding, node_emb) for node_emb in self.node_embeddings]\n",
        "\n",
        "        best_idx = np.argmax(similarities)\n",
        "\n",
        "        return self.node_names[best_idx], similarities[best_idx]\n",
        "\n",
        "    def fetch_related_info(self, node_name):\n",
        "      \"\"\"\n",
        "      Fetches related nodes (1 to 2 hops) connected to the given node_name.\n",
        "      Returns a list of dicts: [{\"info\": str, \"labels\": list}, ...]\n",
        "      \"\"\"\n",
        "      def fetch_related_info_tx(tx, node_name):\n",
        "          query = \"\"\"\n",
        "          MATCH (n)\n",
        "          WHERE toLower(n.name) CONTAINS toLower($node_name)\n",
        "            OR toLower(n.number) CONTAINS toLower($node_name)\n",
        "          WITH n\n",
        "          MATCH (n)-[*1..2]-(related)\n",
        "          RETURN DISTINCT coalesce(related.name, related.number, related.description, '') AS info,\n",
        "                          labels(related) AS labels\n",
        "          \"\"\"\n",
        "          result = tx.run(query, node_name=node_name)\n",
        "          return [{\"info\": record[\"info\"], \"labels\": record[\"labels\"]} for record in result]\n",
        "\n",
        "\n",
        "      with self.driver.session() as session:\n",
        "          return session.read_transaction(fetch_related_info_tx, node_name)\n",
        "\n",
        "\n",
        "\n",
        "    def get_context_text_for_llm(self, node_name):\n",
        "      \"\"\"\n",
        "      Given a node name, fetch related info and combine their 'info' texts into one string,\n",
        "      suitable as context input for an LLM.\n",
        "      \"\"\"\n",
        "      related_infos = self.fetch_related_info(node_name)\n",
        "\n",
        "      if not related_infos:\n",
        "          print(f\"[DEBUG] No related info found for node: {node_name}\")\n",
        "          return f\"No context found for: {node_name}\"\n",
        "\n",
        "      context_texts = [item[\"info\"] for item in related_infos if item[\"info\"] and item[\"info\"].strip() != \"\"]\n",
        "\n",
        "      if not context_texts:\n",
        "          print(f\"[DEBUG] Related nodes found but no valid 'info' properties for: {node_name}\")\n",
        "          return f\"No usable info for: {node_name}\"\n",
        "\n",
        "      context =  \"\\n\".join(context_texts)\n",
        "      # print(\">>>>>>>>>>>>>>\", context)\n",
        "      return context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2aaef49-12e6-4498-9c1c-1d2e0e13f081",
      "metadata": {
        "collapsed": true,
        "id": "c2aaef49-12e6-4498-9c1c-1d2e0e13f081"
      },
      "outputs": [],
      "source": [
        "PDF = PDFFunctions()\n",
        "\n",
        "# chunking\n",
        "chunks = PDF.pdf_to_chunks(pdf_paths)\n",
        "\n",
        "# vec = Vectorize()\n",
        "# # vectorizing\n",
        "# # storing\n",
        "# index = vec.documents_to_vector(chunks)\n",
        "\n",
        "# get tuples\n",
        "infer = Inference()\n",
        "tuples = infer.extract_custom_tuples(chunks)\n",
        "\n",
        "# store tupeles\n",
        "infer.save_tuples_to_file(tuples)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create knowledge graph\n",
        "graph = Graphclass()\n",
        "graph.create_knowledge_graph(tuples)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8xEdlPQKAviY"
      },
      "id": "8xEdlPQKAviY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def split_think_sections(llm_output):\n",
        "    # Find all <think>...</think> blocks\n",
        "    think_matches = re.findall(r\"<think>(.*?)</think>\", llm_output, re.DOTALL)\n",
        "\n",
        "    # Join all think sections (in case there are multiple)\n",
        "    think_text = \"\\n\\n\".join(think_matches).strip()\n",
        "\n",
        "    # Remove <think>...</think> blocks from the full text\n",
        "    non_think_text = re.sub(r\"<think>.*?</think>\", \"\", llm_output, flags=re.DOTALL).strip()\n",
        "\n",
        "    return think_text, non_think_text"
      ],
      "metadata": {
        "id": "lLRw117rnvsX"
      },
      "id": "lLRw117rnvsX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# graph2 = Graphclass()\n",
        "# context_for_llm = graph2.get_context_text_for_llm(\"Attempt to commit culpable homicide\")\n",
        "# print(\"--->>>\", context_for_llm)"
      ],
      "metadata": {
        "id": "U10rKZybAycO"
      },
      "id": "U10rKZybAycO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a350481-2280-4fd3-ab91-b4f2c26a7535",
      "metadata": {
        "id": "8a350481-2280-4fd3-ab91-b4f2c26a7535"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import traceback\n",
        "\n",
        "def process_input_stream(news_article):\n",
        "    logs = \"\"\n",
        "    try:\n",
        "        logs += \"Starting process...\\n\"\n",
        "        yield \"\", logs, \"\"\n",
        "\n",
        "        # Step 1: Find most similar node\n",
        "        logs += \"Finding most similar node...\\n\"\n",
        "        yield \"\", logs, \"\"\n",
        "        similar_node = graph.find_most_similar_node(news_article)\n",
        "        node_name = similar_node[0]\n",
        "        similarity_score = similar_node[1]\n",
        "        logs += f\"Similar Node Found: {node_name} (Score: {similarity_score:.2f})\\n\"\n",
        "        yield \"\", logs, \"\"\n",
        "\n",
        "        # Step 2: Get context for LLM\n",
        "        logs += \"Fetching context for LLM...\\n\"\n",
        "        yield \"\", logs, \"\"\n",
        "        context = graph.get_context_text_for_llm(node_name)\n",
        "        print(\".....................\", context)\n",
        "        logs += \"Context fetched successfully.\\n\"\n",
        "        logs += \"---------- Retrieved Context ----------\\n\"\n",
        "        logs += context + \"\\n\"\n",
        "        logs += \"---------------------------------------\\n\"\n",
        "        yield \"\", logs, \"\"\n",
        "\n",
        "        # Step 3: Call inference engine\n",
        "        logs += \"Running inference...\\n\"\n",
        "        yield \"\", logs, \"\"\n",
        "        answer = infer.answer_user_question(context, similar_node)\n",
        "        answer = split_think_sections(answer)[1]\n",
        "        logs += \"Inference completed.\\n\"\n",
        "        yield \"\", logs, \"\"\n",
        "\n",
        "        # Final output\n",
        "        output = f\"**Answer:**\\n{answer}\"\n",
        "        logs += \"Done.\\n\"\n",
        "        yield \"\", logs, output\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ Error: {str(e)}\\n{traceback.format_exc()}\"\n",
        "        logs += error_msg + \"\\n\"\n",
        "        yield \"\", logs, \"\"\n",
        "\n",
        "# Gradio UI with streaming logs\n",
        "with gr.Blocks(title=\"Legal Assistant: News Article to Law Interpretation\") as demo:\n",
        "    gr.Markdown(\"## 📰 News Article Input\")\n",
        "    input_box = gr.Textbox(lines=6, label=\"Enter News Article\")\n",
        "\n",
        "    gr.Markdown(\"## ⚙️ Under the Hood (Status for Nerds)\")\n",
        "    logs_box = gr.Textbox(lines=18, interactive=False, show_copy_button=True)\n",
        "\n",
        "    gr.Markdown(\"## 🧠 Output (LLM Interpretation)\")\n",
        "    output_box = gr.Markdown()\n",
        "\n",
        "    submit_btn = gr.Button(\"🔍 Analyze\")\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=process_input_stream,\n",
        "        inputs=input_box,\n",
        "        outputs=[input_box, logs_box, output_box],\n",
        "        concurrency_limit=1\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3951acb3-f9b6-436d-83c5-d9751fc70deb",
      "metadata": {
        "id": "3951acb3-f9b6-436d-83c5-d9751fc70deb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}